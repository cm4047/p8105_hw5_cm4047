---
title: "HW5"
author: "Chen Mo"
date: "11/16/2020"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1  

First, load the data.  
```{r}
homicide_data = 
        read_csv("./data/homicide-data.csv")
```
The raw data contains `r ncol(homicide_data)` columns and `r nrow(homicide_data)` rows in total. The data provides information of more than 52,000 criminal homicides over past decades in 50 of the largest American cities. The data includes the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim.  

Then, clean the data.    
```{r}
homicide_df = 
        homicide_data %>%
        janitor::clean_names() %>% 
        mutate(
                city_state = str_c(city, state, sep = ", "),
                disposition = recode(disposition, "Closed without arrest" = "Unsolved"),
                disposition = recode(disposition, "Open/No arrest" = "Unsolved"),
                disposition = recode(disposition, "Closed by arrest" = "Solved")
        ) %>%
        select(city_state, disposition) %>% 
        filter(city_state != "Tulsa, AL")
```
Obtain the total number of homicides and the number of unsolved homicides.  
```{r}
homicide_total = 
        homicide_df %>% 
        group_by(city_state) %>% 
        summarise(
                hom_total = n(),
                hom_unsolved = sum(disposition == "Unsolved")
        )
```
Estimate the proportion of homicides that are unsolved in the city of Baltimore, MD.
```{r}
prop.test(
        homicide_total %>% filter(city_state == "Baltimore, MD") %>% pull(hom_unsolved),
        homicide_total %>% filter(city_state == "Baltimore, MD") %>% pull(hom_total)
) %>%
        broom::tidy() %>% 
        select(estimate, conf.low, conf.high)
```
Estimate the proportion of homicides that are unsolved in all cities.  
```{r}
results_df = 
        homicide_total %>% 
        mutate(
                prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
                tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
        ) %>% 
        select(-prop_tests) %>% 
        unnest(tidy_tests) %>% 
        select(city_state, estimate, conf.low, conf.high)
```
Creat a plot.  
```{r}
results_df %>% 
        mutate(
                city_state = fct_reorder(city_state, estimate)
        ) %>% 
        ggplot(aes(x = city_state, y = estimate)) +
        geom_point() +
        geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

# Problem 2  
Load the data and clean it.     
```{r, message=FALSE, warning=FALSE}
path_df = 
        tibble(
                path = list.files("data/control and experimental")
        ) %>%
        separate(path, into = c("arm", "subject_ID", sep = 4)) %>% 
        mutate(
                arm = recode(arm, "con" = "control arm", "exp" = "experimental arm"),
                path = list.files("data/control and experimental"),
                path = str_c("./data/control and experimental/", path),
                data = map(.x = path, ~read_csv(.x))
        ) %>%
        select(arm, subject_ID, data) %>% 
        unnest(data) %>%
        pivot_longer(
                week_1:week_8,
                names_to = "week",
                values_to = "observations",
                names_prefix = "week_")
```

Make a spaghetti plot showing observations on each subject over time.
```{r}
path_df %>% 
        mutate(subject_ID = str_c(arm, subject_ID)) %>% 
        ggplot(aes(x = week, y = observations, color = arm, group = subject_ID)) +
        geom_point() +
        geom_line() + 
        labs(title = "Observations for each subjects over 8 weeks",
             subtitle = "Control arm VS Experimental arm",
             y = "Values of observations")
```

I also create a plot which shows values of observations by different arm on each subject over time.  
```{r}
path_df %>% 
        ggplot(aes(x = week, y = observations, color = subject_ID, group = subject_ID)) +
        geom_point(aes(shape = arm)) +
        geom_line() + 
        facet_grid(.~arm) +
        labs(title = "Observations for each subjects over 8 weeks",
             y = "Values of observations")
```

From the plots above, we can see that the values of observations of the experimental group have an increasing trend while the values of observations of the control group fluctuated a lot. The average value of observations of the experimental group is larger than that of observations of the control group. Therefore, the intervention was effective in increasing the values of observation which is the outcome of the interest.

# Problem 3  
First, create a function
```{r}
t_test_fun = function(n = 30, mu, sigma = 5){
        rnorm(n = n, mean = mu, sd = sigma) %>% 
                t.test() %>% 
                broom::tidy() %>% 
                select(estimate, p.value) %>% 
                mutate(actual_mu = mu)
}

```

Then generate the simulation when mu equals to 0.  
```{r}
set.seed(3)
sim_result_0 = rerun(5000, t_test_fun(n = 30, mu = 0, sigma = 5)) %>% 
        bind_rows()
```

Generate the simulation when mu equals to 0, 1, 2, 3, 4, 5, and 6.  
```{r}
output = vector("list", 7)
mu_vec = c(0:6)
set.seed(3)
for (i in 1:7) {
        output[[i]] = rerun(5000, t_test_fun(n = 30, mu = mu_vec[[i]], sigma = 5)) 
}
sim_result = bind_rows(output)
```

Make the plot showing the proportion of times the null was rejected.  
```{r}
sim_result %>% 
        group_by(actual_mu) %>% 
        summarise(
                total_number = n(),
                total_reject = sum(p.value < 0.05)
        ) %>% 
        mutate(proportion = total_reject/total_number) %>% 
        ggplot(aes(x = actual_mu, y = proportion)) + 
        geom_point() + 
        geom_smooth(se = FALSE)
```

From the plot above, we can find that when the actual value of $\mu$ increases(effect size of $\mu$), the power also increases. 
This observation aligns with what we expect since when the actual value of $\mu$ increases, it is more probable that we can notice the difference between the value and the null, thus it is more likely to get a lower p-value, and consequently reject the null.

Make a plot showing the average estimate of $\hat \mu$ on the y-axis and the true value of $\mu$ on the x-axis. Overlay a second plot of the average estimate of $\hat \mu$ only in samples for which the null was rejected on the y axis and the true value of $\mu$ on the x axis:  
```{r}
sim_reject = 
        sim_result %>% 
        filter(p.value < 0.05) %>% 
        group_by(actual_mu) %>% 
        summarise(average_estimate = mean(estimate))
sim_result %>%
        group_by(actual_mu) %>% 
        summarise(
                average_estimate = mean(estimate)
        ) %>% 
        ggplot(aes(x = actual_mu, y = average_estimate)) + 
        geom_point(size = 2.0) +
        geom_line() +
        geom_point(data = sim_reject, color = "purple", size = 0.5) +
        geom_line(data = sim_reject, color = "purple")
```

From the plot above, we can see that the sample average of μ̂ across tests for which the null is rejected does not approximately equal to the true value of μ when μ = {1,2 ,3 ,4 ,5, 6}.  

When the true value of μ is equal to 0, since the number of sample size is 5,000 which is large, the estimate of μ̂ forms a normal distribution.Thus, the average estimate of μ̂ and the average estimate of μ̂ when the null was rejected is approximately equal to 0.  

When the true value of μ is equal to 1, the average estimate of μ̂ is approximately 1. The average estimate of μ̂ when the null was rejected is larger than 1 since the upper limit and the lower limit of reject criteria is same, but the number of estimate value of μ which is higher than the upper limit is larger than the number of estimate value of μ which is lower than the lower limit.   

When the true value of μ continutes to increase from 1 to 6, the difference between the average estimate of μ̂ and the average estimate of μ̂ when the null was rejected is smaller since the number of values when the null was rejected increases, the number of value when the null was not rejected decreases.

